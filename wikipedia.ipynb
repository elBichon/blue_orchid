{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanedouard-rgz/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import bs4 as bs \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category(language, subject):\n",
    "    wikipedia.set_lang(language)\n",
    "    source = urllib.request.urlopen(wikipedia.page(subject).url).read()\n",
    "    soup = bs.BeautifulSoup(source,'lxml')\n",
    "    soup_txt = str(soup.body)\n",
    "    category = []\n",
    "    for each_span in soup.find_all('span', {'class':'mw-headline'}):\n",
    "        soup = BeautifulSoup(str(each_span).replace(' ','_'), \"html.parser\").getText()\n",
    "        category.append(soup)\n",
    "    return category\n",
    "\n",
    "def get_data(language, subject):\n",
    "    wikipedia.set_lang(language)\n",
    "    source = urllib.request.urlopen(wikipedia.page(subject).url).read()\n",
    "    soup = bs.BeautifulSoup(source,'lxml')\n",
    "    soup_txt = str(soup.body)\n",
    "    div = []\n",
    "    for each_span in soup.find_all('span', {'class':'mw-headline'}):\n",
    "        str(each_span).replace(' ','_')\n",
    "        div.append(str(each_span))\n",
    "    filter_tag = []\n",
    "    i = 0\n",
    "    while i < len(div)-1:\n",
    "        start = div[i]\n",
    "        end = div[i+1]\n",
    "        text = soup_txt[soup_txt.find(start)+len(start):soup_txt.rfind(end)]\n",
    "        soup = str(BeautifulSoup(text, \"html.parser\"))\n",
    "        soup = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", soup)\n",
    "        soup = BeautifulSoup(soup, \"html.parser\")\n",
    "        soup = re.compile(r'<img.*?/>').sub('', str(soup.find_all('p')))\n",
    "        soup = BeautifulSoup(soup, \"html.parser\")\n",
    "        soup = (re.sub(\"[^a-zA-Z,.;:!0-9]\",\" \",soup.getText()).replace('[','').replace(']','').lstrip().rstrip().lower())    \n",
    "        clean_text = re.sub(' +', ' ',soup).replace(',',' ')\n",
    "        filter_tag.append(clean_text)        \n",
    "        i += 1\n",
    "    return filter_tag\n",
    "\n",
    "\n",
    "def get_len_list(filter_tag):\n",
    "    filtered_text = []\n",
    "    len_list = []\n",
    "    i = 0 \n",
    "    while i < len(filter_tag):\n",
    "        doc = nlp(filter_tag[i])\n",
    "        text = [sent.string.strip() for sent in doc.sents]\n",
    "        filtered_text.append(text)\n",
    "        len_list.append(len(filtered_text[i]))\n",
    "        i += 1\n",
    "    return filtered_text,len_list\n",
    "\n",
    "def generate_dataset(len_list, category):\n",
    "    i = 0\n",
    "    label_list = []\n",
    "    while i < len(len_list):\n",
    "        j = 0\n",
    "        if len_list[i] != 0:\n",
    "            while j != len_list[i]:\n",
    "                label_list.append(category[i].lower())\n",
    "                j += 1\n",
    "        i += 1\n",
    "    flat_list = [item for sublist in  get_len_list(get_data(language, subject))[0] for item in sublist]\n",
    "    data = {'text': flat_list,'label': label_list}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "    print(df.head())\n",
    "    print('Repartition of labels:', df['label'].iloc[0])\n",
    "    print('Data Shape:', df.shape)\n",
    "    return df\n",
    "\n",
    "def get_stop_words(language):\n",
    "    if language == 'en':\n",
    "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    if language == 'fr':\n",
    "        spacy_stopwords = spacy.lang.fr.stop_words.STOP_WORDS\n",
    "    if language == 'de':\n",
    "        spacy_stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
    "    if language == 'es':\n",
    "        spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
    "    if language == 'pt':\n",
    "        spacy_stopwords = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "    if language == 'it':\n",
    "        spacy_stopwords = spacy.lang.it.stop_words.STOP_WORDS\n",
    "    if language == 'nl':\n",
    "        spacy_stopwords = spacy.lang.nl.stop_words.STOP_WORDS\n",
    "    return spacy_stopwords\n",
    "\n",
    "def clean_dataset(language, df):\n",
    "    srce_labels = df.label.values.tolist()\n",
    "    srce_text = df.text.values.tolist()\n",
    "    spacy_stopwords = get_stop_words(language)\n",
    "    clean_text = []\n",
    "    text = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(srce_text):\n",
    "        extract = []\n",
    "        doc = nlp(srce_text[i])\n",
    "        for token in doc:\n",
    "            extract.append(token.lemma_)\n",
    "        clean_text.append(\",\".join(extract).replace(\",\",\" \").replace(\"   \",\" \"))\n",
    "        i += 1\n",
    "    print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "    i = 0\n",
    "    while i < len(clean_text):\n",
    "        doc = nlp(clean_text[i])\n",
    "        tokens = [token.text for token in doc if not token.is_stop]\n",
    "        text.append(\",\".join(tokens).replace(\",\",\" \").replace(\"  \",\" \").replace(\"    \",\" \").replace(\"-PRON-\",\" \").rstrip().lstrip())\n",
    "        i += 1\n",
    "    data = {'text': text,'label': srce_labels}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def create_model(df):\n",
    "    print(\"Creating the bag of words...\\n\")\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 5000) \n",
    "    train_data_features = vectorizer.fit_transform(df.text.values.tolist())\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    print(train_data_features.shape)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    dist = np.sum(train_data_features, axis=0)\n",
    "    print(\"Training the random forest...\")\n",
    "    forest = RandomForestClassifier(n_estimators = 100) \n",
    "    forest = forest.fit(train_data_features, df[\"label\"])\n",
    "    filename = language+\"_\"+subject.replace(\" \",\"_\")+'.sav'\n",
    "    pickle.dump(forest, open(filename, 'wb'))\n",
    "    print('saving model as: '+filename)\n",
    "    \n",
    "def call_model(csv_file, model_file, text):\n",
    "    extract = []\n",
    "    clean_text = []\n",
    "    clean_test_reviews = []\n",
    "    spacy_stopwords = get_stop_words(language)\n",
    "    print(\"Creating the bag of words...\\n\")\n",
    "    df = pd.read_csv(csv_file) \n",
    "    df = df.dropna()\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 5000) \n",
    "    train_data_features = vectorizer.fit_transform(df.text.values.tolist())\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    print(train_data_features.shape)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    dist = np.sum(train_data_features, axis=0)\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        extract.append(token.lemma_)\n",
    "        clean_text.append(\",\".join(extract).replace(\",\",\" \").replace(\"   \",\" \"))\n",
    "    print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "    i = 0\n",
    "    while i < len(clean_text):\n",
    "        doc = nlp(clean_text[i])\n",
    "        tokens = [token.text for token in doc if not token.is_stop]\n",
    "        i += 1\n",
    "    clean_test_reviews.append(\",\".join(tokens).replace(\",\",\" \").replace(\"  \",\" \").replace(\"    \",\" \").replace(\"-PRON-\",\" \").rstrip().lstrip())\n",
    "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "    test_data_features = test_data_features.toarray()\n",
    "    loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "    result = loaded_model.predict(test_data_features)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text\n",
      "0  history  hussars originated in mercenary units of exile...\n",
      "1  history  serbian lancers  also called racowie  were use...\n",
      "2  history  the oldest mention of hussars in polish docume...\n",
      "3  history  in the 15th century  light hussars based on th...\n",
      "4  history  the polish hussars were originally based on th...\n",
      "Repartition of labels: history\n",
      "Data Shape: (91, 2)\n",
      "Number of stop words: 305\n",
      "saving dataframe as:  en_polish_hussars.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAD9CAYAAABHsPWxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFjhJREFUeJzt3Xm0ZWV95vHvI6CQgDJd6YpoMCyX\nQzQUnYKgGEU0LmJiwCxstW1FYyztLAcSNdpJa5xDlgNtMDGNQ0AXDigSbeJEkEGIAaqgmFFU0CA0\nlAMKthopfv3HfksPxb1Vp6D2PfXe+n7WOuvu8+7pd8++9zx7eM8+qSokSVKf7jXrAiRJ0t1nkEuS\n1DGDXJKkjhnkkiR1zCCXJKljBrkkSR0zyCVJ6phBLklSxwxySZI6tv2sC5jGnnvuWfvss8+sy5Ak\naVGsXr36O1U1N820XQT5Pvvsw6pVq2ZdhiRJiyLJN6ed1lPrkiR1zCCXJKljBrkkSR0zyCVJ6phB\nLklSxwxySZI6ZpBLktQxg1ySpI4Z5JIkdayLO7ttjt981QdnXcKSt/ptz511CZKkxiNySZI6NlqQ\nJ9kxyQVJLklyRZI3tPYTklybZE17LB+rBkmSlroxT63/FDi0qm5LsgNwbpLPtnGvqqpPjLhuSZK2\nCaMFeVUVcFt7ukN71FjrkyRpWzTqNfIk2yVZA9wMnF5V57dRb0lyaZJjk9xnzBokSVrKRg3yqlpX\nVcuBvYEDkzwS+B/Aw4ADgN2BV883b5KVSVYlWbV27doxy5QkqVuL0mu9qm4BzgIOq6oba/BT4B+B\nAxeY5/iqWlFVK+bm5hajTEmSujNmr/W5JLu24Z2AJwFXJ1nW2gIcAVw+Vg2SJC11Y/ZaXwacmGQ7\nhh2Gk6vqtCRfTDIHBFgDvHjEGiRJWtLG7LV+KbD/PO2HjrVOSZK2Nd7ZTZKkjhnkkiR1zCCXJKlj\nBrkkSR0zyCVJ6phBLklSxwxySZI6ZpBLktQxg1ySpI4Z5JIkdcwglySpYwa5JEkdG/Pbz6TN9q03\nPmrWJSx5D3rdZbMuQdIW5BG5JEkdM8glSeqYQS5JUscMckmSOmaQS5LUMYNckqSOGeSSJHVstCBP\nsmOSC5JckuSKJG9o7Q9Ocn6Sa5J8LMm9x6pBkqSlbswj8p8Ch1bVfsBy4LAkBwF/AxxbVQ8Bvg+8\nYMQaJEla0kYL8hrc1p7u0B4FHAp8orWfCBwxVg2SJC11o14jT7JdkjXAzcDpwNeBW6rq9jbJ9cAD\nFph3ZZJVSVatXbt2zDIlSerWqEFeVeuqajmwN3Ag8PD5Jltg3uOrakVVrZibmxuzTEmSurUovdar\n6hbgLOAgYNck67+sZW/ghsWoQZKkpWjMXutzSXZtwzsBTwKuAs4EjmyTHQV8aqwaJEla6sb8GtNl\nwIlJtmPYYTi5qk5LciXw0SRvBi4G3j9iDZIkLWmjBXlVXQrsP0/7Nxiul0uSpHvIO7tJktQxg1yS\npI4Z5JIkdcwglySpYwa5JEkdM8glSeqYQS5JUscMckmSOmaQS5LUMYNckqSOGeSSJHXMIJckqWMG\nuSRJHTPIJUnqmEEuSVLHDHJJkjpmkEuS1DGDXJKkjhnkkiR1bLQgT/LAJGcmuSrJFUle3tpfn+Tb\nSda0x1PGqkGSpKVu+xGXfTvwiqq6KMkuwOokp7dxx1bV20dctyRJ24TRgryqbgRubMO3JrkKeMBY\n65MkaVu0KNfIk+wD7A+c35pekuTSJB9Istti1CBJ0lI0epAn2Rk4BTi6qn4IvAfYF1jOcMT+jgXm\nW5lkVZJVa9euHbtMSZK6NGqQJ9mBIcRPqqpPAlTVTVW1rqruAN4LHDjfvFV1fFWtqKoVc3NzY5Yp\nSVK3xuy1HuD9wFVV9c6J9mUTkz0NuHysGiRJWurG7LV+MPAc4LIka1rbXwDPSrIcKOA64EUj1iBJ\n0pI2Zq/1c4HMM+ozY61TkqRtjXd2kySpYwa5JEkdM8glSeqYQS5JUscMckmSOmaQS5LUMYNckqSO\nGeSSJHXMIJckqWMGuSRJHTPIJUnqmEEuSVLHDHJJkjpmkEuS1DGDXJKkjhnkkiR1zCCXJKljBrkk\nSR0zyCVJ6phBLklSx0YL8iQPTHJmkquSXJHk5a199ySnJ7mm/dxtrBokSVrqtt/YyCR/uLHxVfXJ\njYy+HXhFVV2UZBdgdZLTgecBZ1TVMUleA7wGePXmlS1JkmATQQ48dSPjClgwyKvqRuDGNnxrkquA\nBwCHA4e0yU4EzsIglyTpbtlokFfV87fESpLsA+wPnA/s1UKeqroxyf23xDokSdoWTXWNPMleSd6f\n5LPt+SOSvGDKeXcGTgGOrqofTltYkpVJViVZtXbt2mlnkyRpmzJtZ7cTgM8Dv9KefxU4elMzJdmB\nIcRPmrieflOSZW38MuDm+eatquOrakVVrZibm5uyTEmSti3TBvmeVXUycAdAVd0OrNvYDEkCvB+4\nqqreOTHq08BRbfgo4FObVbEkSfq5TXV2W+9HSfZg6OBGkoOAH2xinoOB5wCXJVnT2v4COAY4uZ2a\n/xbw9M2uWpIkAdMH+Z8xHEnvm+Q8YA44cmMzVNW5QBYY/cSpK5QkSQuaKsjbZ8EfDzyUIZy/UlU/\nG7UySZK0SVMFeZIdgT8BHstwev1LSf6hqn4yZnGS+nHwcQfPuoRtwnkvPW/WJWgrM+2p9Q8CtwLH\ntefPAj6E17clSZqpaYP8oVW138TzM5NcMkZBkiRpetN+/Ozi1lMdgCS/BXh+R5KkGdvUl6ZcxnBN\nfAfguUm+1Z7/KnDl+OVJkqSN2dSp9d9flCokSdLdsqkvTfnm5PP2BSc7jlqRJEma2rRfmvIHSa4B\nrgXOBq4DPjtiXZIkaQrTdnZ7E3AQ8NWqejDDndns7CZJ0oxNG+Q/q6rvAvdKcq+qOhNYPmJdkiRp\nCtN+jvyW9r3i5wAnJbkZuH28siRJ0jSmPSI/HPgx8KfA54CvA08dqyhJkjSdab805UcTT08cqRZJ\nkrSZNnVDmFtp30G+4Sigquq+o1QlSZKmsqnPke+yWIVIkqTNN+01ckmStBUyyCVJ6phBLklSxwxy\nSZI6NlqQJ/lAkpuTXD7R9vok306ypj2eMtb6JUnaFox5RH4CcNg87cdW1fL2+MyI65ckackbLcir\n6hzge2MtX5IkzeYa+UuSXNpOve82g/VLkrRkLHaQvwfYl+Gb024E3rHQhElWJlmVZNXatWsXqz5J\nkrqyqEFeVTdV1bqqugN4L3DgRqY9vqpWVNWKubm5xStSkqSOLGqQJ1k28fRpwOULTStJkjZt2u8j\n32xJPgIcAuyZ5Hrgr4BDkixn+CKW64AXjbV+SZK2BaMFeVU9a57m94+1PkmStkXe2U2SpI4Z5JIk\ndcwglySpYwa5JEkdM8glSeqYQS5JUscMckmSOmaQS5LUMYNckqSOGeSSJHXMIJckqWMGuSRJHTPI\nJUnqmEEuSVLHDHJJkjpmkEuS1DGDXJKkjhnkkiR1zCCXJKljBrkkSR0bLciTfCDJzUkun2jbPcnp\nSa5pP3cba/2SJG0LxjwiPwE4bIO21wBnVNVDgDPac0mSdDeNFuRVdQ7wvQ2aDwdObMMnAkeMtX5J\nkrYFi32NfK+quhGg/bz/QhMmWZlkVZJVa9euXbQCJUnqyVbb2a2qjq+qFVW1Ym5ubtblSJK0VVrs\nIL8pyTKA9vPmRV6/JElLymIH+aeBo9rwUcCnFnn9kiQtKWN+/OwjwJeBhya5PskLgGOA30lyDfA7\n7bkkSbqbth9rwVX1rAVGPXGsdUqStK3Zaju7SZKkTTPIJUnqmEEuSVLHDHJJkjpmkEuS1DGDXJKk\njhnkkiR1zCCXJKljBrkkSR0zyCVJ6phBLklSxwxySZI6ZpBLktQxg1ySpI4Z5JIkdcwglySpYwa5\nJEkdM8glSeqYQS5JUse2n8VKk1wH3AqsA26vqhWzqEOSpN7NJMibJ1TVd2a4fkmSuuepdUmSOjar\nIC/gC0lWJ1k5oxokSererE6tH1xVNyS5P3B6kqur6pzJCVrArwR40IMeNIsaJUna6s3kiLyqbmg/\nbwZOBQ6cZ5rjq2pFVa2Ym5tb7BIlSerCogd5kl9Ossv6YeDJwOWLXYckSUvBLE6t7wWcmmT9+j9c\nVZ+bQR2SJHVv0YO8qr4B7LfY65UkaSny42eSJHVsljeEkSRtJc5+3ONnXcKS9/hzzh5luR6RS5LU\nMYNckqSOGeSSJHXMIJckqWMGuSRJHTPIJUnqmEEuSVLHDHJJkjpmkEuS1DGDXJKkjhnkkiR1zCCX\nJKljBrkkSR0zyCVJ6phBLklSxwxySZI6ZpBLktQxg1ySpI7NJMiTHJbkK0m+luQ1s6hBkqSlYNGD\nPMl2wN8Bvws8AnhWkkcsdh2SJC0FszgiPxD4WlV9o6r+A/gocPgM6pAkqXuzCPIHAP8+8fz61iZJ\nkjbT9jNYZ+Zpq7tMlKwEVrantyX5yqhVzdaewHdmXcS08vajZl3C1qSrbQfAX833L7jN6m775WVu\nvwl9bb9s1rb71WknnEWQXw88cOL53sANG05UVccDxy9WUbOUZFVVrZh1Hdp8bru+uf365vYbzOLU\n+oXAQ5I8OMm9gWcCn55BHZIkdW/Rj8ir6vYkLwE+D2wHfKCqrljsOiRJWgpmcWqdqvoM8JlZrHsr\ntU1cQlii3HZ9c/v1ze0HpOou/cwkSVInvEWrJEkdM8jvoST7JLl8nvY3JnnSRuY7wjvazV6SXZP8\nyd2c907bcFPbXNKdJblt1jUsBQb5SKrqdVX1LxuZ5AiGW9ROLclM+jQscbsCdyvI2WAbTrHNtZVJ\nckKSI2ddh3RPGORbxnZJ3pvkiiRfSLLT5BtEkmOSXJnk0iRvT/IY4A+AtyVZk2TfJMuT/Fub5tQk\nu7V5z0ry1iRnA3+Z5NokO7Rx901y3frnuluOAfZt2+HYJGckuSjJZUl+fuvgJM9t2+aSJB9aYBtO\nbvMDkvxrm/6CJLsk+fU2vKYt6yEz+p23mPbdCUtmPb1aCq9PklclubD9b7xhov21Sa5OcnqSjyR5\nZWt/YZv+kiSnJPml1r5Xew+9pD0ek+RNSV4+scy3JHnZ4v+WI6kqH/fgAewD3A4sb89PBv4bcAJw\nJLA78BV+0bFw1/bzBODIieVcCjy+Db8R+F9t+Czg7yem+0fgiDa8EnjHrF+Dnh9t+13ehrcH7tuG\n9wS+xnAnwl9v23DPNm73Bbbh+m1+b+AbwAGt/b5t2ccBz25t9wZ2mvXvP8Xr80/AauAKYGVru639\njZ4PPBa4Dngr8GVgFfCfGT5e+nXgxW2eAG8DLgcuA57R2g8BTptY37uB57Xh64DXAecCz1ygvhcy\n3JviEuAU4JcmtsXfAv/atsWRE3W8G7gS+GeGT88cucCyDwQ+2YYPB37cttuOwDda+77A59pr9CXg\nYa39qe31uRj4F2Cv1v564EPAF4FrgBdO8fqcBXwCuBo4iV+8l0y+Pn8JXDRR+0OA1bP++5ni7+u2\n9vPJDD3Qw3CAeRrwOGAFsAbYCdilvWavbPPsMbGcNwMvbcMfA45uw9sB92P4P7+otd2r/W3usRi/\n42I8PFW7ZVxbVWva8GqGP5r1fgj8BHhfkn9m+AO9kyT3Ywj4s1vTicDHJyb52MTw+4A/Z3iDfT7D\nG5m2jABvTfI44A6G7wDYCzgU+ERVfQegqr63ieU8FLixqi5s0/8QIMmXGc6q7M0QENeM82tsUX9U\nVd9LshNwYZJTgF9m2Pl5HUCG207+e1U9OsmxDCF6MEPgXQH8A/CHwHJgP4adpAuTnDPF+n9SVY/d\nyPhPVtV7Wx1vBl7AsMMEsIxhR+NhDDed+gTwNIbt8yiGbXsl8IEFln0RsH8b/m2GkD2AYafs/NZ+\nPMPOyjVJfgv4e4a/l3OBg6qqkvwxw//sK9o8vwEcxPA6XtzeFx7Nwq/P/gw7kzcA5zG8tudu+Pok\neVKS5e296PkM26EXT26Pi9vznRl2RnYBPlVVPwZI8n8m5nlk2+a7tuk/39oPBZ4LUFXrgB8AP0jy\n3ST7M2z3i6vqu+P+SovHIN8yfjoxvI5h7xH4+Q1wDgSeyHAXu5cw/KFtjh9NLO+81sHu8cB2VXWX\njna6254NzAG/WVU/S3IdQxiFeb4PYCPmnb6qPpzkfOD3gM8n+eOq+uI9L3tUL0vytDb8QIY313UM\nR7+T1t+d8TJg56q6Fbg1yU+S7MoQqB9pb6w3tUtFBzDs6G7MxzYxfqE3c4B/qqo7gCuT7NXaHjdR\nxw1JFnz92//u15I8nOHo/J1t/u2ALyXZGXgM8PH84h7a92k/9wY+lmQZw1H8tROLXh9MP05yZlv2\nxl6fC6rqeoAkaxgOFNYH+YY7+c9P8mfAM9pyexHgr6vqf9+pMfnTjcxzAsPZyUuSPI/h7MXGvA94\nHvCfWHjnrUteIx9Z+2e/Xw03wTmaYa8b4FaGvU2q6gfA95P8dhv3HODsDZc14YPARxhOs+ue+fl2\nYDgFd3ML8Sfwiy8tOAP4L0n2AEiy+zzzTroa+JUkB7Tpd0myfZJfYzgl+7cMwfcbo/xGW0iSQ4An\nAY+uqv0YjpZ2ZDgKXLfB5Ot3Zu/gzju2dzAcMCz0bRG3c+f3oR03GP8jNu4E4CVV9SjgDRvMP1nH\n5Po3Z6fsS8DvAj9jOEX+2PY4h6HuW6pq+cTj4W2+44B3t7petEFdG66/WPj12fD3WMedD8AmX59T\nWq2/z3Bavacjzs8Df9TeL0nygCT3Z9hheWqSHdu435uYZxfgxtZH6NkT7WcA/70tZ7sk923tpwKH\nMewgTe7wdc8gH98uwGlJLmUI5/V7mB8FXpXk4iT7AkcxdJy6lCHs37iRZZ4E7MYQ5roH2pvdeRk+\nQrgcWJFkFcMbw9VtmiuAtwBnJ7mE4cgM7roN1y/zPxiOiI5r05/O8Eb+DODydlT1MIYdsq3Z/YDv\nV9X/S/IwhtPBd9c5wDPaG+scw5HtBcA3gUckuU+7xPTEzVzuQm/mG6vjma2OZcATppj+aODLVbUW\n2INh213RLplcm+TpABns1+a7H/DtNrzh1wUe3oJpD4ajyAtZ+PWZWlX9hCGg3kNnO/lV9QXgw8CX\nk1zGcBlkl3Z56tMMfSA+ydAH4wdtttcyXOI4nfa/2rwceEJbzmqGyxLr/y/PBE6eZ0e0a55av4eq\n6jrgkRPP3z7PZHc5xVVV53HXj5/d5Y2yqg6ZZ3mPZbhme8vm1Kr5VdV/nWKaExn6Lky2bbgNnzcx\n7kLuuj3/uj168TngxW3n8ivAv92DZZ3KcB34EoYj0D+vqv8LkORkhs6e1/CLa6TTWv9m/k2G0/rz\nnSHZsI5D27RfZeNnvmjL3oshaGl13lyt1xTDzsN7kvxPYAeGnbtLGDq1fTzJtxletwdPLPMCho52\nDwLeVFU3JJn39Wk7UJvjJIb+CF/YzPlmoqp2nhh+F/CueSZ7e1W9vvVKPwd4R5v+PQw7LRsu8yaG\nzol3kuReDP+TT98y1W89vEVrZ5Icx3D67ClV9dVZ1yNpeklez9BTe74d/i2x/FcyXMp77RjLn4Uk\nH2bYYd4ROLGqNntnOMONm04DTq2qV2xq+t54RN6ZqnrprGuQtPVpR/X7svmdabdq05wxm2IZVwK/\ntgXK2Sp5RC5pq5fk7xg+djXpXVW1Ra4FtxB88AbNr66qJdUpSkuTQS5JUsfstS5JUscMckmSOmaQ\nS5LUMYNckqSOGeSSJHXs/wMbS6strNVAUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "(91, 535)\n",
      "Training the random forest...\n",
      "saving model as: en_polish_hussars.sav\n"
     ]
    }
   ],
   "source": [
    "language = 'en'\n",
    "subject = 'polish hussars'\n",
    "nlp = spacy.load(language)\n",
    "df = generate_dataset(get_len_list(get_data(language, subject))[1], get_category(language, subject))\n",
    "df = clean_dataset(language, df)\n",
    "file_name = language+\"_\"+subject.replace(\" \",\"_\")+'.csv'\n",
    "print('saving dataframe as: ',file_name)\n",
    "df.to_csv(file_name, sep=',', encoding='utf-8')\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x = df['label'].unique(), y=df['label'].value_counts())\n",
    "plt.show()\n",
    "\n",
    "forest = create_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "(91, 535)\n",
      "Number of stop words: 305\n"
     ]
    }
   ],
   "source": [
    "result = call_model('en_polish_hussars.csv','en_polish_hussars.sav', 'hussar originate mercenary unit exile serbian warrior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['history']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
